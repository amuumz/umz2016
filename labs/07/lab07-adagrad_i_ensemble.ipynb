{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadania - metody optymalizacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 AdaGrad\n",
    "\n",
    "1. Rozszerz poniższą implementację algorytmu SGD o funkcjonalność algorytmu AdaGrad (włączaną parametrem `adaGrad=True`). Zasady działania AdaGrad są podane w materiałach do wykładu.\n",
    "   <br />**Uwaga**: podczas dzielenia przez pierwastek sumy kwadratów historycznych gradientów warto dodać małą wartość $\\epsilon=10^{-7}$ do mianownika, aby uniknąć dzielenia przez 0.\n",
    "   \n",
    "1. Stwórz model wieloklasowej regresji logistycznej na zbiorze MNIST (dla wszystkich 10 cyfr) i oblicz jego jakość na zbiorze testowym. Zastosuj poniższe parametry:\n",
    " 1. Rozmiar wsadu: 50\n",
    " 1. Liczba epok: 2\n",
    " 1. Rozmiar kroku $\\alpha$: 1.0 (w algorytmie AdaGrad $\\alpha$ może być niezmienne)\n",
    "1. Spróbuj uzyskać wynik podobny lub lepszy samym algorytmem SGD bez opcji AdaGrad, dostrajając parametry $\\alpha$. Czy jest to możliwe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Ensemble\n",
    "\n",
    "1. Na danych MNIST wytrenuj 10 klasyfikatorów z sensownie dobranymi parametrami dla algorytmu SGD. Zastosuj randomizację zbioru uczącego dla każdego modelu i w każdej epoce.<br />\n",
    "**Uwaga**: pamiętaj, aby tasować $X$ i $Y$ w tej samej kolejności! \n",
    "1. Oblicz jakość każdego z klasyfikatorów na zbiorze testowym.\n",
    "1. Oblicz jakość predykcji (klas) uzyskanych w wyniku wybranej metody agregacji wyników: głosowania klas lub uśredniania prawdopodobieństw, i porównaj z wcześniej uzyskanymi wynikami. Jak wynik z metody zbiorczej odnosi się do wyników uzyskiwanych przez pojedyncze klasyfikatory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def safeSigmoid(x, eps=0):\n",
    "    y = 1.0/(1.0 + np.exp(-x))\n",
    "    if eps > 0:\n",
    "        y[y < eps] = eps\n",
    "        y[y > 1 - eps] = 1 - eps\n",
    "    return y\n",
    "\n",
    "def h(theta, X, eps=0.0):\n",
    "    return safeSigmoid(X*theta, eps)\n",
    "\n",
    "def J(h,theta,X,y):\n",
    "    m = len(y)\n",
    "    f = h(theta, X, eps=10**-7)\n",
    "    return -np.sum(np.multiply(y, np.log(f)) + \n",
    "                   np.multiply(1 - y, np.log(1 - f)), axis=0)/m\n",
    "\n",
    "def dJ(h,theta,X,y):\n",
    "    return 1.0/len(y)*(X.T*(h(theta,X)-y))\n",
    "\n",
    "def softmax(X):\n",
    "    return np.exp(X)/np.sum(np.exp(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SGD(h, fJ, fdJ, theta, X, y, \n",
    "        alpha=0.001, maxEpochs=1, batchSize=100):\n",
    "    m, n = X.shape\n",
    "    start, end = 0, batchSize\n",
    "    \n",
    "    maxSteps = (m * float(maxEpochs)) / batchSize\n",
    "    for i in range(int(maxSteps)):\n",
    "        XBatch, yBatch =  X[start:end,:], y[start:end,:]\n",
    "\n",
    "        theta = theta - alpha * fdJ(h, theta, XBatch, yBatch)\n",
    "        \n",
    "        if start + batchSize < m:\n",
    "            start += batchSize\n",
    "        else:\n",
    "            start = 0\n",
    "        end = min(start + batchSize, m)\n",
    "        \n",
    "    return theta"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
